{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf18b92-c819-47ec-adc8-695503a609a8",
   "metadata": {},
   "source": [
    "**Gradient Descent Notes(consider formulating to linear algebra):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a6dd8-573f-4ebd-bbae-3ac1545c74ff",
   "metadata": {},
   "source": [
    "For simple linear regression, we define:<br>\n",
    "$y_i = \\theta_{0}+\\theta_{1} x_i + \\epsilon_{i}$\n",
    "<br>\n",
    "$\\implies \\epsilon_i = y_i - (\\theta_0 + \\theta_1 x_i)$\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c541f7-cec5-41ff-878b-db2c3b3b41fe",
   "metadata": {},
   "source": [
    "The loss function (L) is defined for least squares regression by:<br>\n",
    "$L = \\sum_{i=1}^{n} \\epsilon_i^2 = \\sum_{i=1}^{n} ( y_i - (\\theta_0 + \\theta_1 x_i) ) ^2$\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cb397-e1bc-43c9-8eff-2ced55aeb63d",
   "metadata": {},
   "source": [
    "To determine the parameters that minimize this loss we take the gradient and set it equal to zero:\n",
    "<br>\n",
    "$\\nabla L = \\frac{\\partial L}{\\partial  \\theta_0}\\hat{\\theta_0} + \\frac{\\partial L}{\\partial \\theta_1} \\hat{\\theta_1}=  <\\frac{\\partial L}{\\partial  \\theta_0},\\frac{\\partial L}{\\partial  \\theta_1}> = 0$\n",
    "<br>\n",
    "$\\frac{\\partial L}{\\partial  \\theta_0} = \\frac{\\partial}{\\partial \\theta_0} \\sum_{i=1}^{n} (y_i-(\\theta_0 + \\theta_1 x_i))^2 = -2 \\sum_{i=1}^{n} (y_i-\\theta_0 - \\theta_1 x_i)) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dab71-f92f-4df5-8fd7-39bdfacedc7d",
   "metadata": {},
   "source": [
    "$\\implies \\sum_{i=1}^{n} (\\theta_0 + \\theta_1 x_i - y_i) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45437c-70f2-4eb2-bb51-8752289a9aeb",
   "metadata": {},
   "source": [
    "$\\implies n\\theta_0 + b \\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} y_i = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853eb54-3405-4672-bad5-398b6b82dc92",
   "metadata": {},
   "source": [
    "Because $\\bar{x} = \\frac{\\sum_{i=1}^{n}}{n} \\implies \\sum_{i=1}^{n} x_i = n\\bar{x}$,\n",
    "<br>\n",
    "Likewise for $\\sum_{i=1}^{n}$,\n",
    "<br>\n",
    "$\\implies n\\theta_0 + bn\\bar{x} - n\\bar{y} = 0$\n",
    "<br>\n",
    "$\\implies \\theta_0 + b\\bar{x} - \\bar{y} = 0$\n",
    "<br>\n",
    "$\\implies \\theta_0 = \\bar{y} - b\\bar{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593747f9-3eb6-456e-9a27-bab41391f2f7",
   "metadata": {},
   "source": [
    "Similarly,\n",
    "<br>\n",
    "$\\frac{ \\partial L }{ \\partial \\theta_1 } = 0 $\n",
    "<br>\n",
    "$\\implies \\theta_1 = \\frac{ \\sum_{i=1}^{n}(x_i - \\bar{x})(y_i-\\bar{y}) }{ \\sum_{i=1}^{n} (x_i - \\bar{x})^2 }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae16d6-e0fc-4856-a605-9f2f6c4f3370",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What is the goal of this post?(Outline):\n",
    "This outline has been brought to you by: https://www.grammarly.com/blog/how-to-write-a-blog/?q=esl&placement=&&utm_source=google&utm_medium=cpc&utm_campaign=10273012991&utm_targetid=aud-332861653181:dsa-1233402314764&gclid=Cj0KCQjwkbuKBhDRARIsAALysV6NvIqOgvQKXeg2ymq8MBtbUlUMVWO6J-XExD1lsv4Fib0iUpQ5TKcaAnT3EALw_wcB&gclsrc=aw.dshttps://www.grammarly.com/blog/how-to-write-a-blog/?q=esl&placement=&&utm_source=google&utm_medium=cpc&utm_campaign=10273012991&utm_targetid=aud-332861653181:dsa-1233402314764&gclid=Cj0KCQjwkbuKBhDRARIsAALysV6NvIqOgvQKXeg2ymq8MBtbUlUMVWO6J-XExD1lsv4Fib0iUpQ5TKcaAnT3EALw_wcB&gclsrc=aw.ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7aaca6-ae0e-40ed-9b98-2c548ec5c24c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Simple Linear Regression: 6 Methods from Scratch in Python with Example Use Case Study**\n",
    "- What is SLR?\n",
    "  \n",
    "- How does it work? (An overview that includes the introduction of the term 'estimation techniques'\n",
    "  \n",
    "- Estimation Techniques\n",
    "  \n",
    "- SLR Method 1 - the Linear Algebra Method\n",
    "  (metholdology goes here)\n",
    "  \n",
    "- SLR Method 2 - the QR Decomposition Method\n",
    "  (metholdology goes here)(metholdology goes here)  https://genomicsclass.github.io/book/pages/qr_and_regression.htmlhttps://genomicsclass.github.io/book/pages/qr_and_regression.html\n",
    "  \n",
    "- SLR Method 3 - the SVD Decomposition Method\n",
    "  (metholdology goes here)(metholdology goes here)\n",
    "  \n",
    "- SLR Method 4 - the Gradient Descent Method\n",
    "  \n",
    "  \n",
    "- SLR Method 5 - the Sklearn Method\n",
    "  \n",
    "- SLR Method 6 - the Covariance Method\n",
    "  \n",
    "- Which to use when?\n",
    "  \n",
    "- Example Case Study\n",
    "  (choose a dataset, determine which is best SLR method to use, make sure the dataset is of single feature data..maybe impact data from accelerometer, maybe something more common, maybe particle physics data)\n",
    "  (use the following link as a resource of what you might want to include on your analysis: http://genomicsclass.github.io/book/http://genomicsclass.github.io/book/, also make sure to include exploratory analysis, data cleaning, etc..but do not include things outside of the scope of the tutorial ...stick to 'choosing the right slr method for this case study' and 'use of that slr method in this analysis' primarily...use heavy use of visualizations etc to craft a solid argument about the analysis...readup on analysis stuff to present this properly)\n",
    "  \n",
    "NOTE: The following topics/subtopics are outside of the scope of this blogpost...\n",
    "- tuning the $\\alpha$ parameter (a quick argument for what to choose and how can be added in the SLR Method 4 section)\n",
    "- optimization and execution time analysis (a quick plot showing the comparison without methodology, deep description etc. to aid the audience can be included)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2db72f-3bb2-42e6-bed1-72d15d7875a2",
   "metadata": {},
   "source": [
    "**FINAL NOTE**\n",
    "1. Make sure to do the writing, storytelling, and explanatory part of the post SEPARATE from the development/coding of the post.\n",
    "2. Make sure the coding for the post is done without the use of double dollar signs because it will not render properly in the wordpress latex system. Ditto for using the \\limits command in latex, it doesnt work.\n",
    "3. Make sure to review how to write the latex code edits to go from jupyter notebooks latex to wordpress latex - consider writing a script to process the json file as needed for the final uploading to wordpress blog\n",
    "4. Make any hand edits necessary and consider updating the processor code to handle any new procedures that need to be processed all at once in a script in the future (see 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce887a-94b5-4637-9d98-37b24d890b1f",
   "metadata": {},
   "source": [
    "<hr style='border:solid 10px; border-color:black'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068001f0-244b-4c31-8a11-c76c2c2185d8",
   "metadata": {},
   "source": [
    "## **Studies in Machine Learning: Episode 1 - Univariate Linear Regression from Scratch in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ffd48-d9f9-4017-be5b-d17597dbf68a",
   "metadata": {},
   "source": [
    "#### **Overview**\n",
    "Over the course of this blog post, I am going to...\n",
    "- Explain the basic concepts of Univariate Linear Regression\n",
    "- Develop the code for this in 6 methods from scratch starting from theoretical principles\n",
    "- Give suggestions on how to choose the right method for real world application\n",
    "- Provide an example use case study with a real world dataset\n",
    "\n",
    "After understanding this information, the reader will be able to formulate some linear regression algorithms, code them from scratch, select an appropriate ULR algorithm for a dataset, and apply that algorithm to the dataset.\n",
    "\n",
    "This blog post assumes the reader has programming skills in python and mathematics skills that include linear algebra and some vector calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e5b5a-3f1f-4748-bda8-06d5b365127b",
   "metadata": {},
   "source": [
    "#### **What is Univariate Linear Regression?**\n",
    "\n",
    "<i>Univariate Linear Regression</i>(ULR) also known as <i>Simple Linear Regression</i> is a model that fits a linear function to an observable feature graphed against a target variable. It is univariate because only a single feature is used to model the output variable. A model that fits a linear function of multiple observable features the same way is <i>Multivariate Linear Regression.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f62e83-eeb6-44f3-b008-b311d2ea4827",
   "metadata": {},
   "source": [
    "#### **How does it work?**\n",
    "\n",
    "(**In Progress**)<br>\n",
    "In order to better understand how ULR works, lets have a look at a real world dataset.\n",
    "(see https://www.kaggle.com/rezaunderfit/instagram-fake-and-real-accounts-dataset)\n",
    "otherwise go with global warming, but it must be live data so as to use regression a pefectly lined up set of linear datapoints genned from a model)\n",
    "\n",
    "Just introduce the dataset and relevant information at this point, dont do everything here, save the rest of the analysis for after the methods have been developed\n",
    "\n",
    "##### Estimation Techniques\n",
    "\n",
    "- Ordinary Least Squares\n",
    "- Weighted Least Squares\n",
    "- Generalized Least Squares\n",
    "- Maximum liklihood estimation\n",
    "- Ridge Regression\n",
    "- Least Absolute Deviation\n",
    "- Adaptive estimation\n",
    "- Bayesian linear regression\n",
    "- quantile regression\n",
    "- mixed models\n",
    "- principle component regression\n",
    "- least angle regression\n",
    "- theil-sen estimator\n",
    "- alpha trimmed mean\n",
    "- etc\n",
    "- GRADIENT DESCENT\n",
    "More on estimators here: https://en.wikipedia.org/wiki/Linear_regression\n",
    "and here: https://en.wikipedia.org/wiki/List_of_algorithms#Optimization_algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3dcbc8-d8f1-4748-abf6-2f1e1139d49f",
   "metadata": {},
   "source": [
    "#### **Method 1 - Mean Squared Regression** \n",
    "\n",
    "We will look at the mean squares method using linear algebra first. Therefore, our hypothesis function is the equation of a line defined from algebra as:\n",
    "<br><br>\n",
    "$$ y = mx + b$$ \n",
    "is the same as,\n",
    "$$ h(x) = \\theta_1 x + \\theta_0  $$\n",
    "<div style=\"text-align: right\"> (1) </div>\n",
    "<br>\n",
    "However, when trying to fit a linear model to data points, there will also always exist some error between those the true values and predicted values. This is known as the residual error. Including the residual error and looking at the ith dependant value, (1) becomes:\n",
    "<br>\n",
    "$$y_i = \\theta_1 x_i + \\theta_0 + e_i$$ where $e_i$ is the ith residual error.\n",
    "<br><br>\n",
    "If we rearrange this equation, then we see that the ith residual error value is:<br>\n",
    "$$e_i = (\\theta_1 x_1 + \\theta_0) - y_i$$<br>\n",
    "<br>\n",
    "The total residual error is then:<br>\n",
    "$$e = \\sum_{i=1}^n{e_i} = \\sum_{i=1}^n(\\theta_1 x_i + \\theta_0) - y_i$$\n",
    "\n",
    "Given $n$ datapoints, let $\\bf{X}$ be the $(2 \\times n)$ design matrix:<br>\n",
    "$$\\bf{X} = \n",
    "\\begin{bmatrix}\n",
    "1 \\ x_1 \\\\\n",
    "1 \\ x_2 \\\\\n",
    "... \\ ... \\\\\n",
    "1 \\ x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and $\\boldsymbol{\\theta}$ be the $(2 \\times 1)$parameter matrix\n",
    "$$\n",
    "\\boldsymbol{\\theta} = \\vec{\\theta} =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and $\\textbf{e}$ be the $(n\\times 1)$ residual error matrix:\n",
    "$$\n",
    "\\textbf{e} = \\vec{e} =\n",
    "\\begin{bmatrix}\n",
    "e_1 \\\\\n",
    "e_2 \\\\\n",
    "\\dots \\\\\n",
    "e_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and $\\textbf{Y}$ be the $(n \\times 1)$ matrix of actual values:\n",
    "$$\n",
    "\\textbf{Y} = \\vec{y} =\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\dots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Then,our original equation in matrix form looks like:<br>\n",
    "$$ \\textbf{Y} = \\textbf{X} \\boldsymbol{\\theta} + \\textbf{e} $$ <div style=\"text-align: right\"> (2) </div> where \n",
    "\n",
    "$$ \\bf{X}\\boldsymbol{\\theta} =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 + \\theta_1 x_1 \\\\\n",
    "\\theta_0 + \\theta_1 x_2 \\\\\n",
    "\\dots \\\\\n",
    "\\theta_0 + \\theta_1 x_n \\\\\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "<div style=\"text-align: right\"> (3) </div>\n",
    "\n",
    "This implies that the residual error is the following:<br>\n",
    "$$ \\textbf{e} = \\textbf{X}\\boldsymbol{\\theta} - \\textbf{Y} $$\n",
    "\n",
    "\n",
    "In order for $\\textbf{e}$ to have defined values, given $\\textbf{X}$ and $\\textbf{Y}$, we need to determine the coefficients or parameters $\\vec{\\theta}$. There are several estimation techniques available to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11649ebb-70c9-4241-aee7-3de1a0acbfec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
