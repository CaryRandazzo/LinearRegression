{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Studies in Machine Learning - Episode 1: Univariate Linear Regression from Scratch in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "Over the course of this blog post, I am going to...\n",
    "- Explain the basic concepts of Univariate Linear Regression\n",
    "- Develop the code for this in 6 methods from scratch starting from theoretical principles\n",
    "- Give suggestions on how to choose the right method for real world application\n",
    "- Provide an example use case study with a real world dataset\n",
    "\n",
    "After understanding this information, the reader will be able to formulate some linear regression algorithms, code them from scratch, select an appropriate ULR algorithm for a dataset, and apply that algorithm to the dataset.\n",
    "\n",
    "This blog post assumes the reader has programming skills in python and mathematics skills that include linear algebra and some calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is Univariate Linear Regression?**\n",
    "\n",
    "<i>Univariate Linear Regression</i>(ULR) also known as <i>Simple Linear Regression</i> is a model that fits a linear function to an observable feature graphed against a target variable. It is univariate because only a single feature is used to model the output variable. A model that fits a linear function using multiple observable features is <i>Multivariate Linear Regression.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How does it work?**\n",
    "\n",
    "Generally, ULR assumes we know the form of a mathematically linear function. From real world data, we have a list of input data and a list of output data. If the data is truly linear in nature, a linear regression model will be a good fit for the dataset. That <i>linear fit</i> is calculated by assuming there is some error between the dataset output values and the values predicted by a linear model. We could very quickly come up with a terrible linear fit to a dataset.\n",
    "\n",
    "From studies in linear algebra, we should already be familiar with the form of a linear model that has a single dependent variable. This model is the familiar slope intercept form equation which looks like this:\n",
    "<br><br>\n",
    "$ y = mx + b $\n",
    "<br><br>\n",
    "\n",
    "For a very bad <b>fit</b>(this means very bad values for $m$ and $b$), given\n",
    "<br><br>\n",
    "$x = [1,2,3,4,5]$ and $y = [2,4,6,8,10]$\n",
    "<br><br>\n",
    "we can see that assuming\n",
    "<br><br>\n",
    "$y = 1000x + 25565$\n",
    "<br><br>\n",
    "is really bad because our line will start at an excessively high value for $y$ and have an extremely tall slope. We would say that the error or distance from the datapoints generated by the regression line to the datapoints in our dataset will be very high (the x,y data's true regression line is at $y = 2x+2$). \n",
    "<br><br>\n",
    "Then the question becomes, <b>how do we make this error as small as possible?</b> That is, how do we minimize the error between our model's datapoints and our dataset's datapoints? This is where we can begin introducing the various solution methods to achieve this regression line. The first method we will discuss is the Mean Squared Estimation method. Before that, let's formally define the rest of our theory so we can jump right into the solution methods.\n",
    "\n",
    "\n",
    "\n",
    "Starting from the first equation, we get the hypothesis function $h(x)$ for this model:\n",
    "<br><br>\n",
    "$ h(x) = \\theta_1 x + \\theta_0  $\n",
    "<!-- <div style=\"text-align: right\"> (1) </div> -->\n",
    "\n",
    "<br>\n",
    "In supervised machine learning, we define the <b>hypothesis function</b> as the function that best models the target values.\n",
    "\n",
    "<br>\n",
    "These will be our predicted values and we can define them in terms of predicted value $y^{(p)}$ where $y^{(p)}_i$ is the ith predicted value as:\n",
    "<br><br>\n",
    "$y^{(p)}_i = \\theta_1 x_i + \\theta_0$\n",
    "<br><br>\n",
    "and $x_i$ is the ith independent value. The values for $x_i$ come directly from our dataset.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Next, we define the true dependent values as $y^{(t)}$ where $y^{(t)}_i$ is the ith true dependent value. This also comes directly from our dataset.\n",
    "<br><br>\n",
    "\n",
    "When trying to fit a linear model to data points, there will also always exist some error between those the true values and predicted values. This is known as the residual error. We say that the predicted depedent value is the true dependent value plus some residual error. In mathematical form:\n",
    "<br><br>\n",
    "$ y^{(p)}_i = y^{(t)}_i + e_i$\n",
    "<br>\n",
    "$ \\implies \\theta_1 x_i + \\theta_0 = y^{(t)} + e_i$\n",
    "<br><br>\n",
    "where $e_i$ is the ith residual error.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "If we rearrange this equation, then we see that the ith residual error value is:<br>\n",
    "<br>\n",
    "$ e_i = y^{(p)}_i - y^{(t)}_i$\n",
    "<br>\n",
    "$ = (\\theta_1 x_1 + \\theta_0) - y^{(t)}_i$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1 - Mean Squared Estimation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Starting with the Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Notation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now introduce these concepts and further develop them in matrix notation.\n",
    "<br>\n",
    "Note: boldface type $\\textbf{A}$ indicates matrix $\\textbf{A}$ and $\\textbf{A} \\textbf{B}$ side by side assumes a dot product between those matrices such that $\\textbf{A} \\textbf{B} = \\textbf{A} \\cdot \\textbf{B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $n$ datapoints coming directly from our dataset, let $\\bf{X}$ be the $(2 \\times n)$ design matrix:\n",
    "<br><br>\n",
    "$\\bf{X} = \n",
    "\\begin{bmatrix}\n",
    "1 \\ x_1 \\\\\n",
    "1 \\ x_2 \\\\\n",
    "... \\ ... \\\\\n",
    "1 \\ x_n\n",
    "\\end{bmatrix}\n",
    "$\n",
    "<br><Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\boldsymbol{\\theta}$ be the $(2 \\times 1)$parameter matrix:\n",
    "<br><br>\n",
    "$\n",
    "\\boldsymbol{\\theta} =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "<br><br>\n",
    "Speficially for ULR, this matrix will have at least 1 parameter for the slope and potentially a second parameter for bias.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\textbf{Y}$ be the $(n \\times 1)$ matrix of true values from our dataset:\n",
    "<br><br>\n",
    "$\n",
    "\\textbf{Y} = \n",
    "\\begin{bmatrix}\n",
    "y_1^{(t)} \\\\\n",
    "y_2^{(t)} \\\\\n",
    "\\dots \\\\\n",
    "y_n^{(t)}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that our predicted value matrix $\\boldsymbol{\\hat{Y}}$ is\n",
    "<br><br>\n",
    "$ \\boldsymbol{\\hat{Y}}=\n",
    "\\begin{bmatrix}\n",
    "y_1^{(p)} \\\\\n",
    "y_2^{(p)} \\\\\n",
    "... \\\\\n",
    "y_i^{(p)}\n",
    "\\end{bmatrix} = \\bf{X}\\boldsymbol{\\theta} =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 + \\theta_1 x_1 \\\\\n",
    "\\theta_0 + \\theta_1 x_2 \\\\\n",
    "\\dots \\\\\n",
    "\\theta_0 + \\theta_1 x_n \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\textbf{e}$ be the $(n\\times 1)$ residual error matrix:\n",
    "<br><br>\n",
    "$\n",
    "\\textbf{e} = \\begin{bmatrix}\n",
    "e_1 \\\\\n",
    "e_2 \\\\\n",
    "... \\\\\n",
    "e_n\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "(\\theta_0+\\theta_1 x_1) - y^{(t)}_1 \\\\\n",
    "(\\theta_0+\\theta_1 x_2) - y^{(t)}_2 \\\\\n",
    "... \\\\\n",
    "(\\theta_0+\\theta_1 x_i) - y^{(t)}_i\n",
    "\\end{bmatrix}\n",
    "$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then,our model in matrix form looks like:\n",
    "<br><br>\n",
    "$ \\textbf{Y} = \\textbf{X} \\boldsymbol{\\theta} + \\textbf{e} $\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that the residual error is \n",
    "<br><br>\n",
    "$ \\textbf{e} = \\boldsymbol{\\hat{Y}}-\\textbf{Y} = \\textbf{X}\\boldsymbol{\\theta} - \\textbf{Y} $\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What then is the associated error for each datapoint $e_i$ since $x_i$ and $y^{(t)}_i$ are given?**\n",
    "<br>\n",
    "To calculate the values inside the $\\textbf{e}$, we are still missing values for our parameters in our parameter matrix $\\boldsymbol{\\theta}$. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What values should we use for these parameters?**\n",
    "<br>\n",
    "If our goal is to have a model that most closely fits the data, then we want to select parameters that give us the smallest residual error possible between the true and predicted dependent values. Said another way, we want to pick the values that minimize the loss.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **The Loss Function ($L_i$)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin the process of finding these parameters, we can define the <b>Loss Function $(L_i)$</b> as the error associated with a single data entry from our dataset. There are various types of loss functions that can be used to begin finding our parameters and one of the most common is the squared loss. The choice of loss function is beyond the scope of this article and should be studied further. For the Mean Squared Estimation Method, the loss function of choice is indeed the least square loss. To get this loss, previously, we defined the residual error as\n",
    "<br><br>\n",
    "$e_i = (\\theta_1 x_i + \\theta_0) - y_i$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Mean Squared Estimation method, we can define $L_i$ as\n",
    "<br><br>\n",
    "$L_i = [y^{(p)}_i - y^{(t)}_i]^2 = [(\\theta_1 x_i + \\theta_0) - y^{(t)}_i]^2$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a helpful formula, the norm of a matrix:\n",
    "<br><br>\n",
    "$ ||\\textbf{a}|| = \\textbf{a}^T \\textbf{a} = \\sum^{n}_{i=1} a_i$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **The Cost Function $(J)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the loss function, we define the <b>Cost Function $(J)$</b> as the error asscoated with our entire dataset. To go from Squared Loss as our loss function to Mean Squared Loss as our cost function, we take the average over all squared errors in our dataset:\n",
    "<br><br>\n",
    "$\n",
    "J = MSE(L_i) =  \\frac{1}{n}\\sum_{i=1}^{n} L_i \n",
    "$\n",
    "\n",
    "$ \n",
    "= \\frac{1}{n}||L|| \n",
    "$\n",
    "\n",
    "$\n",
    "= \\frac{1}{n} ||\\boldsymbol{\\hat{Y}} - \\textbf{Y}||^2 \n",
    "$\n",
    "\n",
    "$\n",
    "=  \\frac{1}{n} ||\\textbf{X} \\boldsymbol{\\theta}-\\textbf{Y}||^2\n",
    "$\n",
    "\n",
    "<br>\n",
    "where $MSE(L_i)$ is the Mean Squared Error function applied over the loss function $L_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in expanded form the mean squared error is\n",
    "<br><br>\n",
    "$ \n",
    "\\frac{1}{n}||\\textbf{X}\\boldsymbol{\\theta} - \\textbf{Y}||^2 = \n",
    "\\frac{1}{n}\n",
    "(\\textbf{X}\\boldsymbol{\\theta}-\\textbf{Y})^T (\\textbf{X}\\boldsymbol{\\theta}-\\textbf{Y})\n",
    "$\n",
    "\n",
    "$ = \\frac{1}{n}\n",
    "(\\textbf{X}^T \\boldsymbol{\\theta}^T \\textbf{X}\\boldsymbol{\\theta} - \n",
    "\\textbf{Y}^T \\textbf{X} \\boldsymbol{\\theta} - \n",
    " \\textbf{Y} \\textbf{X}^T \\boldsymbol{\\theta}^T + \n",
    "\\textbf{Y}^T \\textbf{Y} )\n",
    "$\n",
    "\n",
    "$ = \\frac{1}{n}(\n",
    "\\textbf{X}^T \\boldsymbol{\\theta}^T \\textbf{X}\\boldsymbol{\\theta} -\n",
    "2 \\textbf{Y} \\textbf{X}^T \\boldsymbol{\\theta}^T +\n",
    "\\textbf{Y}^T \\textbf{Y}\n",
    ")\n",
    "$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Minimizing the Cost Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum value of this $MSE(L_i)$ occurs when the derivative of the $MSE(\\textbf{e})$ with respect to the error or when \n",
    "<br><br>\n",
    "$ \\large \\frac{\\partial MSE(L_i)}{\\partial \\boldsymbol{\\theta}} = 0$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies,\n",
    "<br><br>\n",
    "$\n",
    "\\frac{1}{n}\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} (\n",
    "\\textbf{X}^T \\boldsymbol{\\theta}^T \\textbf{X}\\boldsymbol{\\theta} -\n",
    "2 \\textbf{Y} \\textbf{X}^T \\boldsymbol{\\theta}^T +\n",
    "\\textbf{Y}^T \\textbf{Y}\n",
    ") \n",
    "= 0 \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\implies 0 = \n",
    "2 \\textbf{X}^T \\textbf{X} \\boldsymbol{\\theta} - 2 \\textbf{Y} \\textbf{X}^T + 0\n",
    "$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\implies  \\textbf{X}^T \\textbf{X} \\boldsymbol{\\theta} =  \\textbf{Y} \\textbf{X}^T\n",
    "$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\implies  \\boldsymbol{\\theta_{min}} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{Y} \\textbf{X}^T\n",
    "$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "<br>\n",
    "$\n",
    "\\boldsymbol{\\theta_{min}} = \\begin{bmatrix}\n",
    "\\theta_{0_{min}} \\\\\n",
    "\\theta_{1_{min}} \n",
    "\\end{bmatrix}\n",
    "$\n",
    "are the best parameters for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Checking that our result is the minimum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that this value for $\\boldsymbol{\\theta}$ is the minimum, we use <i>the second derivative test</i> from calculus:\n",
    "<br><br>\n",
    "$\n",
    "\\implies \n",
    "\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}\n",
    "(\\textbf{X}^T \\textbf{X} \\boldsymbol{\\theta} - \\textbf{Y} \\textbf{X}^T)\n",
    "= \\textbf{X}^T \\textbf{X}\n",
    "$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since\n",
    "<br><br>\n",
    "$\n",
    " \\textbf{X}^T \\textbf{X}\n",
    "\\gt 0\n",
    "$\n",
    "<br><br>\n",
    "this will always be positive, it is positive definite and is the minimum we seek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, to find our predictions with our now known mimized parameter values, we simply use equation (**?**) and plug in the values as shown here:\n",
    "<br><br>\n",
    "$\n",
    "\\hat{\\textbf{Y}} = \\textbf{X} \\boldsymbol{\\theta_{min}} = \\textbf{X} (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{Y} \\textbf{X}^T\n",
    "$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Constructing the Model in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import our necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T02:08:53.947162Z",
     "start_time": "2021-08-30T02:08:53.352076Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Generating the Data**\n",
    "Instead of starting with a dataset directly, let us generate specified true values with specified input values for a linear model and parameters of our choosing:\n",
    "<br>\n",
    "Let's use $y^{(t)} = 2x_i + 3$ to generate our true values and lets let $x_i$ be a range of values whose length we will call vec_len:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T02:08:57.736666Z",
     "start_time": "2021-08-30T02:08:57.733010Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_data(vec_len):\n",
    "    \"\"\"\n",
    "    Function to generate the random data that will be used for variosu linear regression methods.\n",
    "    \n",
    "    The linear equation, is defined as:\n",
    "        y = th0 + th1*X + e \n",
    "          =   3 +   2*X + e\n",
    "        \n",
    "        y   - vector of dependent variables \n",
    "        x   - vector of independent variables\n",
    "        th0 - constant intercept parameter\n",
    "        th1 - constant slope parameter\n",
    "        e   - vector of residual errors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the design matrix as a column vector of ones of size vec_len and a column vector of random standard normal values of size vec_len\n",
    "    x_0, x_1 = np.ones(vec_len), np.random.standard_normal(vec_len)    \n",
    "    \n",
    "    # Stack the columns together to complete the design matrix\n",
    "    X = np.column_stack((x_0,x_1))\n",
    "    \n",
    "    # Generate the vector that will contain the residual error so our datapoints are all not exactly on the regression line, we multiply by 2 to increase the size of error\n",
    "    e = 2*np.random.standard_normal(vec_len)\n",
    "    \n",
    "    # Return X as the design matrix, and Y as th0(=3) + th1(=2) * X(the second column's values, not the column of 1 values) + residual_error(e)\n",
    "    return X, 3 + 2*X[:,1]+e #X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "X,y = gen_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our generated data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc2a597b9b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,1],y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Defining the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T02:09:00.407700Z",
     "start_time": "2021-08-30T02:09:00.404733Z"
    }
   },
   "outputs": [],
   "source": [
    "def lr(X,y):\n",
    "    \"\"\"\n",
    "    Numpy Mean Square Error Method for Univariate Linear Regression.\n",
    "    \n",
    "    Returns, in order:\n",
    "        theta_0, theta_1, y_predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the array of parameter values as b from the theory previously demonstrated\n",
    "    b = inv( X.T @ X ) @ X.T @ y\n",
    "    \n",
    "    return b[0],b[1], X @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Calculate the parameters and prediction values from the dataset using the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.01433265563995,\n",
       " 2.1136325359745016,\n",
       " array([ 2.06206830e+00,  2.80286962e+00,  5.49482266e+00,  2.76335536e+00,\n",
       "         3.66352164e+00,  4.28402807e+00,  5.46393764e+00,  3.95781978e+00,\n",
       "         6.02893074e+00,  1.36954909e+00, -2.02664019e-02,  1.71172258e+00,\n",
       "         4.03675236e+00,  5.08335538e+00,  8.65192839e-01,  2.76721020e+00,\n",
       "         4.58991374e+00, -2.41698990e-01,  3.41512923e+00,  3.93022480e+00,\n",
       "        -7.73161924e-01,  1.24607171e+00,  3.45039343e+00,  4.10116272e+00,\n",
       "         3.69460942e+00, -2.77635789e+00,  2.31349360e+00,  4.71887585e+00,\n",
       "         2.04599234e+00, -1.59822613e+00,  4.14591639e+00,  3.88629713e+00,\n",
       "        -4.40348364e-01,  1.41229071e+00,  2.34862983e+00,  9.85717843e-01,\n",
       "         3.73377140e+00,  1.75615841e+00,  7.13563095e-01,  7.01168906e+00,\n",
       "         3.66436111e+00,  3.96496581e-01,  1.96177192e+00,  4.21990979e+00,\n",
       "         4.06397289e+00,  2.89144185e+00,  1.79551773e+00,  4.30515625e+00,\n",
       "         2.58200093e+00,  3.49778698e+00,  5.83053794e+00, -2.31982972e+00,\n",
       "         4.05474955e+00,  8.23153267e-01,  2.98571357e+00,  2.42145970e+00,\n",
       "         4.85279479e+00,  5.82675689e+00,  1.79374007e+00,  4.58023270e+00,\n",
       "         6.41647435e+00,  4.44251684e+00,  3.22790528e+00,  1.47562082e+00,\n",
       "         4.10107699e+00,  2.62923565e+00,  2.71723701e+00,  4.39518841e+00,\n",
       "         3.73800773e+00,  4.09759441e+00,  2.58466685e+00,  3.54773810e+00,\n",
       "         1.95210054e+00,  3.12151853e+00,  1.64980223e+00,  1.14880950e+00,\n",
       "         7.87611049e-01,  3.19339499e+00,  2.81135652e+00, -7.99133719e-01,\n",
       "         2.88254934e+00,  2.34522313e+00,  4.46949444e+00,  3.79134397e+00,\n",
       "         7.10476680e-01,  2.40312853e+00,  5.57523679e+00,  4.05465359e+00,\n",
       "         6.70802740e-01,  2.91964430e+00, -6.41613376e-03, -3.06618749e-01,\n",
       "         3.80582358e+00, -6.78448876e-01,  4.87944730e+00,  2.95712199e+00,\n",
       "         3.33758375e+00,  6.07725779e-01,  2.87054830e+00,  9.62987263e-01]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Visualization - our regression line vs our actual datapoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc2a593e160>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of the values from the dataset\n",
    "plt.scatter(X[:,1],y)\n",
    "\n",
    "# Plot of regression line\n",
    "plt.plot(X[:,1],lr(X,y)[2],c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Just because Scientific Programmers get a 'lol' out of this**\n",
    "For a model with these assumptions, the parameters or predicted values could be calculated from numpy with a single line of code.\n",
    "<br>\n",
    "Assuming we already have our data defined from a dataset, or in our case from gen_data,\n",
    "we could either get the parameter values directly by our formula for theta_min:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[theta_0,theta_1]: [3.01433266 2.11363254]\n"
     ]
    }
   ],
   "source": [
    "print('[theta_0,theta_1]:', inv( X.T @ X ) @ X.T @ y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, we can get the yhat prediction values using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_preds: [2.0620683  2.80286962 5.49482266 2.76335536 3.66352164]\n"
     ]
    }
   ],
   "source": [
    "# Displaying only the first 5 predictions of 100\n",
    "print('y_preds:', (X @ (inv( X.T @ X ) @ X.T @ y))[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of course, there are limitations to this approach as not all input shapes in real datasets will behave well with this matrix formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes Chapter 1 on Linear Regression applied by the Method of Mean Square Estimation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
